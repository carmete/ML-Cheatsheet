% ========================================================

\section{Math Prerequisites}

% ========================================================
\begin{itemize}
	\item Bayes rule
	\[
	     p(A, B) = \underbrace{p(A | B)}_{\text{Lik.}} \underbrace{p(B)}_{\text{Prior}} = \underbrace{p(B | A)}_{\text{Post}} \underbrace{p(A)}_{\text{Marg. Lik.}}
	\]
    \item Gaussian distribution
    \begin{myalign*}
        \N(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(- \frac{(x - \mu)^2}{2 \sigma^2})
    \end{myalign*}
    \begin{myalign*}
        \N(\* x | \bm \mu, \bm \Sigma) = \frac{1}{\sqrt{2 \pi |\bm \Sigma|}} \exp(- \frac{1}{2} (\*x - \bm \mu)^T \bm \Sigma^{-1} (\*x - \bm \mu))
    \end{myalign*}
    \item Production of independent variables:
    \begin{myalign*}
        \V(XY) = \E(X^2)\E(Y^2) - [\E(X)]^2 [\E(Y)]^2
    \end{myalign*}
    \item Covariance matrix of a data vector $\*x$
    \begin{myalign*}
        \*\Sigma = \frac{1}{N} \sum_{n = 1}^N (\*x_n - \E(\*x))(\*x_n - \E(\*x))^T
    \end{myalign*}
\end{itemize}

\subsection{Convexity}
\begin{itemize}
\item A function $f(x)$ is convex, if for any $x_1, x_2 \in \* X$ and for any $0 \leq \lambda \leq 1$, we have :
$$ f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$$

\item Strictly convex if the inequality is strict.

\item Sums of convex functions are also convex.

\item A function with the form log-sum-exp is convex.

\item The Hessian is related to the convexity: a twice differentiable function is convex i-o-if the Hessian is positive definite.
\item The Hessian of a convex function is positive semi-definite and for a strictly-convex function it is positive definite.
\begin{myalign*}
      \*H_{i,j} = d^2 f /d x_i dx_j
 \end{myalign*}
\end{itemize}
% ========================================================

\subsection{Linear Algebra}
\begin{itemize}
\item Column $\*x \in \R^n$, rows $\*x^T$, matrix $\*A \in \R^{m \times n}$

\item $\*x^T \*x$ is a scalar, $\*x \*x^T$ is a matrix

\item $\*A^{-1}$ exist if $\*A$ is full rank

\item $(\*A \*B)^T = \*B^T \*A^T$

\item \textbf{Condition number} of a function measures how much the output value can change for a small change in the input. A matrix with a high condition number is said to be \textbf{ill-conditioned}. If $\*A$ is normal ($A^T A = A A^T$) then
\begin{myalign*}
    k(\*A) = 
    \left|
    	\frac{\lambda_{max}(\*A)}{\lambda_{min}(\*A)}
    \right|
\end{myalign*}

\item A positive definite matrix is \textbf{symmetric} with all positive eigenvalues
\item The real symmetric $N \times N$ matrix $\*V$ is said to be \textbf{positive semidefinite} if 
\begin{myalign*}
    \*a^T \*V \*a \geq 0
\end{myalign*}
for any real $N \times 1$ vector $a$.
\item \textbf{positive definite} if $\*a^T \*V \*a > 0$
\item Cost of matrix inversion: $O(n^3) \rightarrow O(n^{2.372})$
\item $det(\*A)$ using LU decomposition: $O(n^3)$

\end{itemize}

% ========================================================
\section{Cost functions}
\begin{itemize}
    \item Cost functions are used to learn parameters that explain the data well.
    \item It is essential to make sure that a global minimum exist $\rightarrow$ lower bounded
\end{itemize}

\textbf{Mean square error (MSE)}:
\begin{myalign*}
    MSE(\bm w ) = \sum^N_{n = 1} (y_n - f(\* x_n ))^2
\end{myalign*}

\begin{itemize}
    \item MSE is \textbf{convex} thus it has only one global minumum value.
    \item MSE is not good when outliers are present.
\end{itemize}

\textbf{Mean Absolute Error (MAE)}:
\begin{myalign*}
    MAE = \sum^N_{n = 1} |y_n - f(\* x_n)|
\end{myalign*}

\textbf{Huber loss}
\begin{myalign*}
    Huber = 
    \left\{ 
        \begin{array}{c c}
            \frac{1}{2} z^2 &,|z| \leq \delta \\
            \delta |z| - \frac{1}{2} \delta^2 & ,|z| > \delta
        \end{array}
    \right.
\end{myalign*}
\begin{itemize}
\item Huber loss is convex, differentiable, and also robust to outliers but hard to set $\delta$.\\
\end{itemize}

\textbf{Tukey's bisquare loss}
\begin{myalign*}
    L(z) =
    \left\{ 
        \begin{array}{c c}
            z(\delta^2 - z^2)^2 &, |z| < \delta \\
            0 &, |z| \geq \delta
        \end{array}
    \right.
\end{myalign*}
Non-convex, non-diff., but robust to outliers.


\textbf{Hinge loss}
\begin{myalign*}
    Hinge = [1 - y_n f(\* x_n)]_+ = \max(0, 1 - y_n f(\* x_n))
\end{myalign*}

\textbf{Logistic loss}
\begin{myalign*}
    Logistic =  \log(1 - \exp(y_n f(\* x_n)))
\end{myalign*}


% ========================================================

\section{Regression}
\begin{itemize}
    \item \textbf{Data} consists of N pairs $(y_n, \*x_n)$
    \begin{enumerate}
        \item $y_n$ the n'th output
        \item $\*x_n$ is a vector of D inputs
    \end{enumerate}
    \item \textbf{Prediction}: predict the ouput for a new input vector.

    \item \textbf{Interpretation}: understand the effect of inputs on output.

    \item \textbf{Outliers} are data that are far away from most of the other examples.
\end{itemize}
% ========================================================

\subsection{Linear Regression}
\begin{itemize}
	\item Model that assume linear relationship between inputs and the ouput.
\begin{myalign*}
  y_n &\approx f(\* x_n) := w_0 + w_1 x_{n1} + ... = \w_0 + \*x^T_n \bm w
\end{myalign*}
with $\bm w$ the parameters of the model.
\item Variance grows only linearly with dimensionality
\end{itemize}

% ========================================================
\section{Optimization}
\subsection{Grid search}
\begin{itemize}
    \item Compute the cost over a grid of $M$ points to find the minimum
    \item Exponential Complexity 
    \item Hard to find a good range of values
\end{itemize}

% ========================================================

\subsection{Gradient Descent}
\begin{itemize}
	\item Gradient descent uses only first-order information and takes steps in the direction of the gradient
	\item Given a cost function $\L(\bm w)$ we wish to find $\bm w$ that minimizes the cost: 
	\begin{myalign*}
	    \min_{\bm w} \L(\bm w)
	\end{myalign*}
\end{itemize}

% ========================================================

\subsection{Batch Gradient Descent}
\begin{itemize}
    \item Take steps in the opposite direction of the gradient
    $$ \bm w^{(t + 1)} \leftarrow \bm w^{(t)} - \gamma \bm \nabla \L(\bm w^{(t)})$$
    with $\gamma > 0$ the learning rate. 
    \item With $\gamma$ too big, method might diverge. With $\gamma$ too small, convergence is slow.
\end{itemize}

% ========================================================

\subsection{Gradients for MSE}
\begin{itemize}
    \item We define the error vector $\* e$:
    \begin{myalign*}
        \*e := \* y - \*{X} \bm w
    \end{myalign*}
    \item and MSE as follows:
    \begin{myalign*}
        \L(\bm w) = \frac{1}{2N} \sum^N_{n = 1} (y_n - \*{\tilde{x}}_n^T \bm w)^2 = \frac{1}{2N} \*e^T \*e
    \end{myalign*}
    \item then the gradient is given by
    \begin{myalign*}
        \bm \nabla \L(\bm w) = - \frac{1}{N} \*{X}^T \*e
    \end{myalign*}

    \item Optimality conditions:
    \begin{enumerate}
        \item \textit{necessary}: gradient equal zero: $\frac{d \L(\bm w^*)}{d \bm w} = 0$
        \item \textit{sufficient}: Hessian matrix is positive definite: $\* H(\bm w^*) = \frac{d^2 \L(\bm w^*)}{d \bm w d \bm w^T}$
    \end{enumerate}

    \item Very sensitive to illconditioning. Therefore, always normalize your feature otherwise step-size selection is difficult since different directions might move at different speed.
    \item \textit{Complexity}: $O(NDI)$ with $I$ the number of iterations
\end{itemize}

% ========================================================

\subsection{Stochastic Gradient Descent}

In ML, most cost functions are formulated as a sum over the training examples:
$$ \L(\bm w) = \frac{1}{N}\sum_{n=1}^N\L_n(\bm w)$$
$\Rightarrow$ SGD algo is given by update rule:
$$ \bm w^{(t + 1)} \leftarrow \bm w^{(t)} - \gamma \bm \nabla \L_n(\bm w^{(t)})$$
\emph{Idea}: Cheap but unbiased estimate of grad. $$\E[\bm\nabla\L_n(\bm w)] = \bm \nabla(\bm w)$$

% ========================================================

\subsection{Mini-batch SGD}

Update direction ($B \subseteq [N]$):
$$\bm g^{(t)} := \frac{1}{|B|} \sum_{n\in B} \bm \nabla \L_n(\bm w^{(t)}) $$ 
Update rule : $ \bm w^{(t + 1)} \leftarrow \bm w^{(t)} - \gamma \bm g^{(t)}$ 

% ========================================================

\subsection{Subgradients (Non-Smooth OPT)}

A vector $\bm g\in \R^D$ s.t.
$$ \L(\bm u) \geq \L(\bm w) + \bm g^T(\bm u - \bm w) \quad \forall \bm u $$
is the subgradient to $\L$ at $\bm w$.
If $\L$ is differentiable at $\bm w$, we have $\bm g = \bm \nabla \L(\bm w)$
% ========================================================

\section{Least Squares}
\begin{itemize}
    \item In some cases, we can compute the minimum of the cost function analytically.

    \item use the first optimality conditions:
    \begin{myalign*}
        \bm \nabla L(\bm w^*)= 0 \Rightarrow \*X^T \*e = \*X^T (\*y - \*X \bm w)=  0
    \end{myalign*}
    \item When $\*X^T\*X$ is invertible, we have the closed-form expression
    \begin{myalign*}
        \bm w^* = (\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item thus we can predict values for a new $\* x_m$
    \begin{myalign*}
        y_m := \*{x^T_m} \bm w^* = \*{x^T_m}(\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item The \textbf{Gram matrix} $\*X^T\*X$ is positive definite and is also invertible iff $\*X$ has full column rank.
    
    \item \textit{Complexity}: $O(ND^2 + D^3) \equiv O(ND^2)$

    \item $\*X$ can be rank deficient when $D > N$ or when the comlumns $\*{\bar{x}}_d$ are nearly collinear. In this case, the matrix is ill-conditioned, leading to numerical issues.
\end{itemize}

% ========================================================

\section{Maximum Likelihood}
\begin{itemize}
    \item Let define our mistakes $\epsilon_n \sim \N(0, \sigma^2)$.
    \begin{myalign*}
        \rightarrow y_n = \*{x}_n^T \bm w + \epsilon_n
    \end{myalign*}    
    \item Another way of expressing this:
    \begin{myalign*}
        p(\*y | \*{X, \bm w}) &= \prod_{n = 1}^N p(y_n | \*{x}_n, \bm w)\\
        &= \prod_{n = 1}^N \N(y_n | \*{x}_n^T \bm w, \sigma^2)
    \end{myalign*}
    which defines the likelihood of observating $\* y$ given $\*X$ and $\bm w$
    \item Define cost with log-likelihood
    \begin{myalign*}
        \L_{lik}(\bm w) &= \log p(\*y | \*X, \bm w)\\
        &= - \frac{1}{2 \sigma^2} \sum^N_{n = 1} (y_n - \*{x}_n^T \bm w)^2 + cnst
    \end{myalign*}
    \item Maximum likelihood estimator (MLE) gives another way to design cost functions
    \begin{myalign*}
        \argmin_{\bm w} \L_{MSE}(\bm w) = \argmax_{\bm w} \L_{lik}(\bm w)
    \end{myalign*}
    \item MLE can also be interpreted as finding the model under which the observed data is most likely to have been generated from.
    \item With Laplace distribution
    \begin{myalign*}
        p(y_n | \*{x}_n, \bm w) = \frac{1}{2b} e^{-\frac{1}{b}|y_n - \*{x}_n^T \bm w|}
    \end{myalign*}
    \begin{myalign*}
        \sum_n \log p(y_n | \*{x}_n, \bm w) = \sum_n |y_n - \*{x}_n^T \bm w| + cnst
    \end{myalign*}

\end{itemize}

% ========================================================

\section{Ridge Regression}
\begin{itemize}
    \item Linear models usually underfit. One way is to use nonlinear basis functions instead.
    \begin{myalign*}
        y_n = w_0 + \sum_{j = 1}^M w_j \phi_j(\*x_n) = \bm{\tilde{\phi}}(\*x_n)^T \bm w
    \end{myalign*}
    \item This model is linear in $\bm w$ but nonlinear in $\*x$. Note that the dimensionality is now M, not D.
    \item Polynomial basis
    \begin{myalign*}
        \bm \phi(x_n) = [1, x_n , x_n^2 , ..., x_n^M]
    \end{myalign*}
    \item The least square solution becomes
    \begin{myalign*}
        \bm w^*_{lse} = (\bm{\tilde{\Phi}}^T\bm{\tilde{\Phi}})^{-1}\bm{\tilde{\Phi}}^T \*y
    \end{myalign*}

    \item Complex models overfit easily. Thus we can choose simpler models by adding a \textbf{regularization term} which penalizes complex models
    \begin{myalign*}
        \min_{\bm w} 
        \left( 
        	\L(\bm w) + \frac{\lambda}{2N} \sum^M_{j = 1} w_j^2 
        \right)
    \end{myalign*}

    \begin{myalign*}
        \bm w^* = \argmin_{\bm w} 
        \left(
        	\frac{1}{2}(\*y - \*X \bm w)^T(\*y - \*X \bm w) + \frac{\lambda}{2} \bm w^T \bm w
        \right)
    \end{myalign*}

    \item Note that $w_0$ is not penalized.
    \item By differentiating and setting to zero we get
    \begin{myalign*}
        \bm w_{ridge} = (\bm{\tilde{\Phi}}^T\bm{\tilde{\Phi}} + \bm{\Lambda})^{-1} \bm{\tilde{\Phi}}^T \*y
    \end{myalign*}
    \begin{myalign*}
        \bm{\Lambda} = 
        \left[
            \begin{array}{c c}
                0 & \underline{0}  \\
                \underline{0} & \lambda I_m
            \end{array}
        \right]
    \end{myalign*}
    \item Ridge regression improves the condition number of the Gram matrix since the eigenvalues of $(\bm{\tilde{\Phi}}^T\bm{\tilde{\Phi}} + \lambda I_m)$ are at least $\lambda$ %LEARN

    \item \textbf{Maximum-a-posteriori (MAP) estimator}: %LEARN
	     \begin{itemize}
	     	\item Maximizes the product of the likelihood \textit{and} the \textbf{prior}.
	     	\begin{myalign*}
	     	     \bm w_{MAP} = \argmax_{\bm w} 
	     	     \left(
	     	     	p(\*y | \*X, \bm \Lambda)p(\bm w | \bm \Sigma)
	     	     \right)
	     	 \end{myalign*} 
	     	\item Assume $w_0 = 0$
	     	\end{itemize}
	     	\end{itemize}
$$
    \bm w_{ridge} = \argmax_{\bm w}
    \left(
	    \log 
	    \left[
		\prod_{n = 1}^N \N(y_n | \*x^T_n \bm w, \bm \Lambda) \times \N(\bm w | 0, \*I)
	    \right]
	\right)
$$
	\begin{itemize}
	\item \textbf{Lasso regularizer} forces some $w_i$ to be strictly 0 and therefore forces sparsity in the model.
	\begin{myalign*}
	    \min_{\bm w} \frac{1}{2 N} \sum_{n = 1}^N(y_n - \bm{\tilde{\phi}}(\*x_n)^T \bm w)^2, \hspace{10pt} \\\text{ such that } \sum_{i = 1}^M |w_i| \leq \tau
	\end{myalign*}
\end{itemize}

% ========================================================

\section{Cross-Validation}
\begin{itemize}
	\item We should choose $\lambda$ to minimize the mistakes that will be made in the future.
	\item We split the data into train and validation sets and we pretend that the validation set is the future data. We fit our model on the training set and compute a prediction-error on the validation set. This gives us an \textit{estimate} of the \textit{generalization error}.
	\item \textbf{K-fold cross validation} randomly partition the data into $K$ groups. We train on $K - 1$ groups and test on the remaining group. We repeat this until we have tested on all $K$ sets. We then average the results.
	\item Cross-validation returns an unbiased estimate of the generalization error and its variance.
\end{itemize}
% ========================================================

\section{Bias-Variance decomposition}
\begin{itemize}
	\item The expected test error can be expressed as the sum of two terms
	\begin{itemize}
	 	\item \textbf{Squared bias}: The average \textit{shift} of the predictions 
	 	\item \textbf{Variance}: measure how data points vary around their average.
	 \end{itemize} 
	 \begin{center}
	 	expected loss $= (\text{bias})^2$ + variance + noise
	 \end{center}
	\item Both model bias and estimation bias are important
	\item Ridge regression increases estimation bias while reducing variance
	\item Increasing model complexity increases test error
	\item Small $\lambda \rightarrow$ low bias but large variance
	\item Large $\lambda \rightarrow$ large bias but low variance
	\begin{myalign*}
	    err = \sigma^2 + E[f_{lse} - E[f_{lse}]]^2 + [f_{true} - E[f_{lse}]]^2
	\end{myalign*}
\end{itemize}
% ========================================================

\section{Logistic Regression}

% ========================================================
\begin{itemize}
	\item \textbf{Classification} relates input variables $\*x$ to discrete output variable $y$
	\item \textbf{Binary classifier}: we use $y = 0$ for $\*C_1$ and $y = 1$ for $\*C_2$.
	\item Can use least-squares to predict $\hat{y}_*$
	\begin{myalign*}
	    \hat{y} = 
	    \left\{
	    	\begin{array}{c c}		
	    		\*C_1 & \hat{y}_* < 0.5 \\
	    		\*C_2 & \hat{y}_* \geq 0.5 \\
	    	\end{array}		
	    \right.
	\end{myalign*}
	\item \textbf{Logistic function}
	\begin{myalign*}
	    \sigma(x) = \frac{\exp(x)}{1 + \exp(x)}
	\end{myalign*}
	\begin{myalign*}
	    p(y_n = \*C_1| \*x_n) = \sigma(\bm x^T \bm w)\\
	    p(y_n = \*C_2 | \*x_n) = 1 - \sigma(\bm x^T \bm w)
	\end{myalign*}
	\item The probabilistic model:
	\begin{myalign*}
	    p(\*y | \*X, \bm w) = \prod_{n = 1}^N \sigma(\bm x_n^T \bm w)^{y_n}(1 - \sigma(\bm x_n^T \bm w))^{1 - y_n}
	\end{myalign*}
	\item The log-likelihood:
	\begin{myalign*}
	    \L_{MLE}(\bm w) = \sum_{n = 1}^N
	    \left(
	    	y_n \bm x_n^T \bm w - \log(1 + \exp(\bm x_n^T \bm w))
	    \right)
	\end{myalign*}
	\item We can use the fact that
	\begin{myalign*}
	    \frac{d}{dx}\log(1 + \exp(x)) = \sigma(x)
	\end{myalign*}
	\item Gradient of the log-likelihood %LEARN
	\begin{myalign*}
	    \*g = \frac{d \L}{d \bm w} &= \sum_{n = 1}^N 
	    \left( 
	    	\bm x_n y_n - \bm x_n \sigma(\bm x_n^T \bm w)
	    	\right) \\
	    &= - \*X^T[\sigma(\*X \bm w) - \*y]
	\end{myalign*}
	\item The negative of the log-likelihood $- \L_{mle}(\bm w)$ is convex
	\item \textbf{Hessian} of the log-likelihood
	\begin{itemize}
		\item We know that
		\begin{myalign*}
		    \frac{d \sigma(t)}{dt} = \sigma(t)(1 - \sigma(t))
		\end{myalign*}
		\item Hessian is the derivative of the gradient
		\begin{myalign*}
		    \*H(\bm w) &= - \frac{d \*g(\bm w)}{d \bm w^T}  = \sum_{n = 1}^N \frac{d}{d \bm w^T} \sigma(\bm x_n^T \bm w) \bm x_n\\
		    &= \sum_{n = 1}^N \bm x_n \sigma(\bm x_n^T \bm w)(1 - \sigma(\bm x_n^T \bm w)) \bm x_n^T\\
		    &= \tilde{\*X}^T \*S \tilde{\*X}
		\end{myalign*}
		where $\*S$ is a $N \times N$ diagonal matrix with diagonals
		\begin{myalign*}
		    S_{nn} = \sigma(\bm x_n^T \bm w)(1 - \sigma(\bm x_n^T \bm w))
		\end{myalign*}
		\item The negative of the log-likelihood is not strictly convex.
	\end{itemize}
	\item \textbf{Newton's Method} %LEARN
	\begin{itemize}
		\item Uses second-order information and takes steps in the direction that minimizes a quadratic approximation
		\begin{myalign*}
		    \L(\bm w) = \L(\bm w^{(k)}) + \*g_k^T (\bm w - \bm w^{(k)})\\ + (\bm w - \bm w^{(k)})^T \*H_k(\bm w - \bm w^{(k)})
		\end{myalign*}
		and it's minimum is at
		\begin{myalign*}
		    \bm w^{k + 1} = \bm w^{(k)} - \alpha_k \*H_k^{-1}\*g_k
		\end{myalign*}
		where $\*g_k$ is the gradient and $\alpha_k$ the learning rate.
		\item Complexity: $O((ND^2 + D^3)I)$
	\end{itemize}
	\item \textbf{Penalized Logistic Regression}
	\begin{myalign*}
	    \min_{\bm w} 
	    \left(
	    	- \sum_{n = 1}^N \log p(y_n | \*x_n^T \bm w) + \lambda \sum_{d = 1}^D w^2_d
	    \right)
	\end{myalign*}
\end{itemize}

% ========================================================
\section{Generalized Linear Model}
\begin{itemize}
	\item \textbf{Exponential family distribution}
	\begin{myalign*}
	    p(\*y | \bm \eta) = \frac{h(y)}{Z} \exp(\bm \eta^T \bm \phi(\*y) - A(\bm \eta))
	\end{myalign*}
	\begin{itemize}
		\item Bernoulli distribution
		\begin{myalign*}
		    p(y | \mu) &= \mu^y (1- \mu)^{1 - y}\\
		    &= \exp(y \log(\frac{\mu}{1 - \mu} + \log(1 - \mu)))
		\end{myalign*}
		\item there is a relationship between $\eta$ and $\mu$ throught the \textbf{link function}
		\begin{myalign*}
		    \eta = \log(\frac{\mu}{1 - \mu}) \leftrightarrow \mu = \frac{e^{\eta}}{1 + e^{\eta}}
		\end{myalign*}
		\item Note that $\mu$ is the mean parameter of $y$
	\end{itemize}
	\item Relationship between the mean $\bm \mu$ and $\bm \eta$ is defined using a link function $g$
	\begin{myalign*}
	    \bm \eta = \*g(\bm \mu) \Leftrightarrow \bm \mu = \*g^{-1}(\bm \eta)
	\end{myalign*}
	\item First and second derivatives of $A(\eta)$ are related to the mean and the variance
	\begin{myalign*}
	    \frac{d A(\eta)}{d \eta} = \E[\bm \phi(\eta)], \hspace{4pt} \frac{d^2 A(\eta)}{d \eta^2} = \V[\bm \phi(\eta)]
	\end{myalign*}
	\item $A(\eta)$ is convex
	\item The generalized maximum likelihood cost to minimize is
	\begin{myalign*}
	    \min_{\bm w} \L(\bm w) = - \sum_{n = 1}^N \log(p(y_n | \bm x^T_n \bm w))
	\end{myalign*}
	where $p(y_n | \bm x^T_n \bm w)$ is an exponential family distribution
	\item We obtain the solution
	\begin{myalign*}
	    \frac{d \L}{d \bm w} = \*X^T[\*g^{-1}(\bm \eta) - \bm \phi(\*y)]
	\end{myalign*}
\end{itemize}

% ========================================================

\section{k-Nearest Neighbor (k-NN)}
\begin{itemize}
	\item The k-NN prediction for $\*x$ is
	\begin{myalign*}
	    f(\*x) = \frac{1}{k} \sum_{\*x_n \in nbh_k(\*x)} y_n
	\end{myalign*}
	where $nbh_k(\*x)$ is the neightborhood of $\*x$ defined by the k closest points $\*x_n$ in the training data
	\item \textbf{Curse of dimensionality}: Generalizing correctly becomes exponentially harder as the dimensionality grows.
	\item Gathering more inputs variables may be a bad thing
\end{itemize}

% ========================================================

\section{Support Vector Machine} 
\begin{itemize}
	\item Combination of the kernel trick plus a modified loss function (Hinge loss)
	\item Solution to the dual problem is sparse and non-zero entries will be our \textbf{support vectors}.
	\item \textbf{Kernelised feature vector} where $\bm \mu_k$ are centroids
	\begin{myalign*}
	    \bm \phi(\*x) = [k(\*x, \bm \mu_1), ..., k(\*x, \bm \mu_K)]
	\end{myalign*}
	\item In practice we'll take a subset of data points to be prototype $\rightarrow$ \textbf{sparse vector machine}.
	\item Assume $y_n \in \{-1, 1\}$
	\item SVM optimizes the following cost
	\begin{myalign*}
	    \L(\bm w) = \min_{\bm w} \sum_{n = 1}^N [1 - y_n \tilde{\bm \phi}_n^T \bm w]_+ + \frac{\lambda}{2} \sum_{j = 1}^M w^2_j
	\end{myalign*}
	\item Minimum doesn't change with a rescaling of $\bm w$
	\item choose the hyperplane so that the distance from it to the nearest data point on each side is maximized
	\item \textbf{Duality}:
	\begin{itemize}
		\item Hard to minimize $g(\bm w)$ so we define
		\begin{myalign*}
		    \L(\bm w) = \max_{\bm \alpha} G(\bm w, \bm \alpha)
		\end{myalign*}
		\item we use the property that
		\begin{myalign*}
		    C[v_n]_+ = \max(0, C v_n) = \max_{\alpha_n \in [0, C]} \alpha_n v_n
		\end{myalign*}
		\item We can rewrite the problem as
		\begin{myalign*}
		    \min_{\bm w} \max_{\bm \alpha \in [0, C]^N} \sum_{n = 1}^N \alpha_n (1 - y_n \bm \phi_n^T \bm w) + \frac{1}{2} \sum_{j = 1}^M w_j^2
		\end{myalign*}
		\item This is differentiable, convex in $\bm w$ and concave in $\bm \alpha$
		\item \textbf{Minimax theorem}: 
		\begin{myalign*}
		    \min_{\bm w} \max_{\bm \alpha} G(\bm w, \bm \alpha) = \max_{\bm \alpha} \min_{\bm w} G(\bm w, \bm \alpha)
		\end{myalign*}
		because $G$ is convex in $\bm w$ and concave in $\bm \alpha$.
		\item Derivative w.r.t. $\bm w$:
		\begin{myalign*}
		    \bm \nabla_{\bm w} G(\bm w, \bm \alpha) = - \sum_{n=1}^N\alpha_n y_n\bm x_n + \lambda \bm w
		\end{myalign*}
		\item Equating this to 0, we get:
			\begin{myalign*}
			  \bm w(\bm \alpha) = \frac{1}{\lambda} \sum_{n=1}^N \alpha_n y_n \bm x_n = \frac{1}{\lambda} \*{X}\*{Y}\bm \alpha \\
			  \*{Y} := \text{diag}(\bm y)
			\end{myalign*}
		\item Plugging $\bm w^*$ back in the dual problem
		\begin{myalign*}
		    \max_{\bm \alpha \in [0, 1]^N} \bm \alpha^T \*1 - \frac{1}{2\lambda} \bm \alpha^T \*Y \bm X^T \bm X \* Y \bm \alpha
		\end{myalign*}
		\item This is a differentiable least-squares problem. Optimization is easy using Sequential Minimal Optimization. It is also naturally kernelized with $\*K = \bm X^T \bm X$
		\item The solution $\bm \alpha$ is sparse and is non-zero only for the training examples that are instrumental in determining the decision boundary.
	\end{itemize}
\end{itemize}

% ========================================================

\section{Kernel Ridge Regression}
\begin{itemize}
	\item The following is true for ridge regression
	\begin{align}
	    \bm w^* &= (\*X^T \*X + \lambda \*I_D)^{-1} \*X^T \*y \\
	    &= \*X^T(\*X\*X^T + \lambda \*I_N)^{-1} \*y = \*X^T \bm \alpha^* \nonumber
	\end{align}
	\item Complexity of computing $\bm w$: (1) $O(D^2 N + D^3)$, (2) $O(D N^2 + N^3)$
	\item Thus we have
	\begin{myalign*}
	    \bm w^* = \*X\bm \alpha^*, \quad \text{with } \bm w^* \in \R^D \text{ and } \bm \alpha^* \in \R^N
	\end{myalign*}
	\item The representer theorem allows us to write an equivalent optimization problem in terms of $\bm \alpha$.
	\begin{myalign*}
	    \bm \alpha = \argmax_{\bm \alpha} 
	    \left(
	    - \frac{1}{2}\bm \alpha^T (\*X^T \*X + \lambda \*I_N) \bm \alpha + \bm \alpha^T \*y 
	    \right)
	\end{myalign*}
	\item $\*K = \*X \*X^T$ is called the \textbf{kernel matrix} or \textbf{Gram matrix}.
	\item If $\*K$ is positive definite, then it's called a \textbf{Mercer Kernel}.
	\item $\*K_{i,j} = k(\*x_i, \*x_j)$
	\item If the kernel is Mercer, then there exists a function $\bm \phi(\*x)$ s.t.
	\begin{myalign*}
	    k(\*x, \*x') = \bm \phi(\*x)^T \bm \phi(\*x')
	\end{myalign*}
	\item \textbf{Kernel trick}: 
	\begin{itemize}
		\item We can work directly with $\*K$ and never have to worry about $\*X$
		\item Replace $\langle \*x, \*x' \rangle$ with $k(\*x, \*x')$.
		\item Kernel function can be interpreted as a measure of similarity
		\item The evaluation of a kernel is usually faster with $k$ than with $\bm \phi$
	\end{itemize}
	\item Kernelized rigde regression might be computationally more efficient in some cases.
	\item \textbf{Radial Basis function kernel (RBF)}
	\begin{myalign*}
	     k(\*x, \*x') = \exp(- \frac{1}{2}(\*x - \*x')^T (\*x - \*x'))
	 \end{myalign*} 
	\item Properties of a kernel to ensure the existance of a corresponding $\bm \phi$:
	\begin{itemize}
		\item $\*K$ should be symmetric: $k(\*x, \*x') = k(\*x', \*x)$
		\item $\* K$ should be positive semidefinite.
	\end{itemize}
	\item Thus we get
	\begin{myalign*}
	    \*y = \bm w^T \*x = \sum_{i = 1}^K \alpha_i \*x_i^T \*x = \sum_{i = 1}^K \alpha_i k(\*x, \*x_i) 
	\end{myalign*}
\end{itemize}

% ========================================================

\section{K-means}
\begin{itemize}
	\item \textbf{Unsupervised learning}: Represent particular input patterns in a way that reflects the statistical structure of the overall collections of input partterns.
	\item \textbf{Cluster} are groups of points whose inter-point distances are small compared to the distances outside the cluster.
	\begin{myalign*}
	    \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = \sum_{k = 1}^K \sum_{n = 1}^N z_{nk} ||\*x_n - \bm \mu_k||^2_2
	\end{myalign*}
	such that $z_{nk} \in \{0, 1\}$ and $\sum_{k = 1}^K z_{nk} = 1$
	\item K-means algorithm (Coordinate Descent): \\
	Initialize $\bm \mu_k$, then iterate
	\begin{enumerate}
		\item For all n, compute $\*z_n$ given $\bm \mu$
		\begin{myalign*}
		    z_{nk} = 
		    \left\{
		    	\begin{array}{c c}
		    		1 & \text{ if } k = \argmin_j || \*x_n - \bm \mu ||^2_2\\
		    		0 & \text{otherwise}
		    	\end{array}
		    \right.
		\end{myalign*}
		\item For all $k$, compute $\mu_k$ given $\*z$
		\begin{myalign*}
		    \bm \mu_k = \frac{\sum_{n = 1}^N z_{nk} \*x_n}{\sum_{n = 1}^N z_{nk}}
		\end{myalign*}
	\end{enumerate}
	\item A good initialization procedure is to choose the prototypes to be equal to a random subset of $K$ data points.
	\item Probabilistic model
	\begin{myalign*}
	    p(\*z, \bm \mu) = \prod_{n = 1}^N \prod_{k = 1}^K 
	    \left[
	    	\N(\*x_n | \bm \mu_k, \*I)
	    \right]^{z_{nk}}
	\end{myalign*}
	\item K-means as a Matrix Factorization
	\begin{myalign*}
	 \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = ||\*X - \*M\*Z^T||_{\text{Frob}}^2
	\end{myalign*}
	\item Computation can be heavy, each example can belong to only on cluster and clusters have to be spherical.
\end{itemize}

% ========================================================

\section{Gaussian Mixture Models}
\begin{itemize}
	\item Clusters can be elliptical using a full covariance matrix instead of isotropic covariance.
	\begin{myalign*}
	    p(\*X | \bm \mu, \bm \Sigma, \*z) = \prod_{n = 1}^N \prod_{k = 1}^K 
		    \left[
		    	\N(\*x_n | \bm \mu_k, \* \bm \Sigma_k)
		    \right]^{z_{nk}}
	\end{myalign*}

	\item \textbf{Soft-clustering}: Points can belong to several cluster by defining $z_n$ to be a random variable.
	\begin{myalign*}
		p(z_{n} = k) &= \pi_k \text{ where } \pi_k > 0, \forall k, %LEARN
		\sum_{k = 1}^K \pi_k = 1
	\end{myalign*}
	\item Joint distribution of Gaussian mixture model
	\begin{myalign*}
	    p(\*X, \*z | \bm \mu, \bm \Sigma, \bm \pi)
	    = \prod_{n = 1}^N
	    	p(\*x_n | \*r_n, \bm \mu, \bm \Sigma) p(\*z_n | \bm \pi) \\
	    = \prod_{n=1}^N \prod_{k = 1}^K [(\N(\*x_n |\bm \mu_k, \bm \Sigma_k))^{z_{nk}}] \prod_{k = 1}^K [\pi]^{z_{nk}}
	\end{myalign*}
	\item $z_n$ are called \textit{latent} unobserved variables
	\item Unknown parameters are given by $ \bm \theta = \{\bm \mu, \bm \Sigma, \bm \pi\}$
	\item We get the \textbf{marginal likelihood} by marginalizing $z_n$ out from the likelihood
	\begin{myalign*}
	    p(\*x_n | \bm \theta) &= \sum_{k = 1}^K p(\*x_n, z_{n} = k | \bm \theta)\\
	    &= \sum_{k = 1}^K p(z_{n} = k | \bm \theta) p(\*x_n | z_{n} = k, \bm \theta)\\
	    &= \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
	\item Without a latent variable model, number of parameters grow at rate $O(N)$
	\item After marginalization, the growth is reduced to $O(D^2 K)$

	\item To get maximum likelihood estimate of $\bm \theta$, we maximize
	\begin{myalign*}
	    \max_{\bm \theta} \sum_{n = 1}^N \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
\end{itemize}

% ========================================================

\section{Expectation Maximization Algorithm} %LEARN

\begin{itemize}
	\item \textit{[ALGORITHM]} Start with $\bm \theta^{(1)}$ and iterate
	\begin{enumerate}
		\item \textit{Expectation step}: Compute a lower bound to the cost such that it is tight at the previous $\bm \theta^{(t)}$
		\begin{comment}
		\begin{myalign*}
		    \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k) \geq \sum_{k = 1}^K \gamma(r_{nk}) \log \frac{\pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}{\gamma(r_{nk})}
		\end{myalign*}
		\end{comment}
		with equality when,
		\begin{myalign*}
		    q_{kn} = \frac{\pi_k \N(\*x_n| \bm \mu_k, \bm \Sigma_k)}{\sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}
		\end{myalign*}
		\item \textit{Maximization step}: Update $\bm \theta$
		\begin{myalign*}
		    \bm \theta^{(t + 1)} = \argmax_{\bm \theta} \L(\bm \theta, \bm \theta^{(t)})
		\end{myalign*}

		\begin{myalign*}
		    \bm \mu_k^{(t + 1)} = \frac{\sum_{n = 1}^N \gamma^{(i)}(r_{nk}) \*x_n}{\sum_{n = 1}^N q_{kn}^{(t)}}
		\end{myalign*}

		\begin{myalign*}
		    \bm\Sigma_k^{(t + 1)} = \frac{\sum_{n = 1}^N q_{kn}^{(t)} (\*x_n - \bm \mu_k^{(t + 1)})(\*x_n - \bm \mu_k^{(t + 1)})^T}{\sum_{n = 1}^N q_{kn}^{(t)})}
		\end{myalign*}

		\begin{myalign*}
		    \pi_k^{(t + 1)} = \frac{1}{N} \sum_{n = 1}^N q_{kn}^{(t)}
		\end{myalign*}
	\end{enumerate}
	\item If covariance is diagonal $\rightarrow$ K-means.
\end{itemize}

% ========================================================

\section{Matrix factorization}
\begin{itemize}
	\item We have $D$ movies and $N$ users
	\item $\*X$ is a matrix $D \times N$ with $x_{dn}$ the rating of n'th user for d'th movie.
	\item We project data vectors $\*x_n$ to a smaller dimension $\*z_n \in \Bbb R^M$
	\item We have now 2 latent variables:
	\begin{itemize}
		\item $\*Z$ a $N \times K$ matrix that gives features for the users
	 	\item $\*W$ a $D \times K$ matrix that gives features for the movies
	 \end{itemize} 
	 \begin{myalign*}
	     x_{dn} \approx \*w_d^T\*z_n
	 \end{myalign*}
	 \item We can add a regularizer and minimize the following cost:
	 \begin{myalign*}
	     \L(\*W, \*Z) = \frac{1}{2}\sum_{(d,n)\in\Omega} [x_{dn} - (\*W\*Z^T)_{dn}]^2 \\ + \frac{\lambda_w}{2} \sum_{d = 1}^D \*w_d^T\*w_d + \frac{\lambda_z}{2} \sum_{n = 1}^N \*z_n^T \*z_n
	 \end{myalign*}
	 \item We can use coordinate descent algorithm, by first minimizing w.r.t. $\*Z$ given $\*W$ and then minimizing $\*W$ given $\*Z$. This is called \textbf{Alternating least-squares (ALS)}:
	 \begin{myalign*}
	     \*Z^T &\leftarrow (\*W^T \*W + \lambda_z \*I_K)^{-1} \*W^T \*X \\
	     \*W^T &\leftarrow (\*Z^T \*Z + \lambda_w \*I_K)^{-1} \*Z^T\*X^T
	 \end{myalign*}
	 \item \textit{Complexity}: $O(D N K^2 + N K^3) \rightarrow O(D N K^2)$
	 \item Probabilistic model
	 \begin{myalign*}
	     \prod_{(d,n)\in\Omega}\N(x_{dn} | \*w_d^T \*z_n, I) 
	     \times \prod_{n = 1}^N \N(\*z_n | 0, \frac{1}{\lambda_z} I) \\
	     \times \prod_{d = 1}^D \N(\*w_d | 0, \frac{1}{\lambda_w} I)
	 \end{myalign*}

	 \item Since many ratings are missing we cannot normalize the data. A solution is to add offset terms:
	 \begin{myalign*}
	     \frac{1}{2} \sum_{(d,n)\in\Omega} (x_{dn} - \*w_d^T \*z_n - w_{0d} - z_{0n} - \mu)^2
	 \end{myalign*}
\end{itemize}
% ========================================================

\section{Singular Value Decomposition}
\begin{itemize}
	\item Matrix factorization method
	\begin{myalign*}
	    \*X = \*U \*S \*V^T
	\end{myalign*}
	\begin{itemize}
		\item $\*U$ is a unitary $D \times D$ matrix
		\item $\*V$ is a unitary $N \times N$ matrix
		\item $\*S$ is a non-negative diagonal matrix of size $D \times N$ which are called \textbf{singular values} appearing in a descending order.
		\item Columns of $\*U$ and $\*V$ are the left and right \textbf{singular vectors} respectively.
	\end{itemize}
	\item Assuming $D < N$ we have
	\begin{myalign*}
	    \*X = \sum_{d = 1}^D s_d \*u_d \*v_d^T
	\end{myalign*}
	This tells you about the spectrum of $\*X$ where higher singular vectors contain the \textit{low-frequency information} and lower singular values contain the \textit{high-frequency information}. 
	\item Dimensionality Reduction \\
	Take the matrix $\*S^{(K)}$ with the $K$ first diagonal elements non zero. Then, rank-$K$ approx:
	$$ \*X \approx \*X_K = \*U\*S^{(K)}\*V^T$$
\end{itemize}
% ========================================================

\subsection{Principal Componement Analysis}
\begin{itemize}
	\item PCA is a dimensionality reduction method and a method to decorrelate the data
$\*X \approx \tilde{\*X} = \*W \*Z^T$
such that columns of $\*W$ are orthogonal.
\item If the data is zero mean
\begin{myalign*}
    \*\Sigma = \frac{1}{N} \*X \*X^T &\Rightarrow \*X \*X^T = \*U \*S^2 \*U^T \\
    \Rightarrow \*U^T \*X \*X^T \*U &= \*U^T \*U \*S^2 \*U^T \*U = \*S^2\\
\end{myalign*}
\item Thus the columns of matrix $\*U$ are called the \textbf{principal components} and they decorrelate the covariance matrix.
\item Using SVD, we can compute the matrices in the following way
\begin{myalign*}
    \*W &= \*U \*S_D^{1 / 2}, \*Z^T = \*S^{1 / 2}\*V^T 
\end{myalign*}
\end{itemize}


% ========================================================

\section{Neural Net}
\begin{itemize}
	\item \textcolor{red}{Coucou}
\end{itemize}


% ========================================================

\section{Bayes Net}
\begin{itemize}
	\item Graph example: $p(x, y, z) = p(y | x) p(z | x) p(x)$ : $(y \leftarrow x \rightarrow z)$ 
	\item \textbf{D-Separation} X and Y are D-separated by Z if every path from $ x \in X$ to $y \in Y$ is blocked by Z.
	\item \textbf{Blocked Path} if the path contains a variable that
	\begin{itemize}
		\item is in Z and is \textbf{head-to-tail} or \textbf{tail-to-tail}
		\item the node is \textbf{head-to-head} and neither the node nor the descendant are in Z.
	\end{itemize}	
	\item \textbf{Markov Blanket} (which blocks node A from the rest of the net) contains:
	\begin{itemize}
		\item parents of A
		\item children of A
		\item parents of children of A
	\end{itemize}
	
\end{itemize}






