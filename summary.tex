% ========================================================
% ========================================================

\section{Math Prerequisites}

\subsection{Derivatives }

\begin{itemize}
    \item $\partial ( \mathbf { X } \mathbf { Y } ) = ( \partial \mathbf { X } ) \mathbf { Y } + \mathbf { X } ( \partial \mathbf { Y } )$
    \item $\frac { \partial \mathbf { f } ( \mathbf { g } ( \mathbf { u } ( \mathbf { x } ) ) ) } { \partial \mathbf { x } } = \frac { \partial \mathbf { u } ( \mathbf { x } ) } { \partial \mathbf { x } } \frac { \partial \mathbf { g } ( \mathbf { u } ) } { \partial \mathbf { u } } \frac { \partial \mathbf { f } ( \mathbf { g } ) } { \partial \mathbf { g } }$
	\item $\frac { \partial \mathbf { x } ^ { T } \mathbf { a } } { \partial \mathbf { x } } = \frac { \partial \mathbf { a } ^ { T } \mathbf { x } } { \partial \mathbf { x } } = \mathbf { a }$
	\item $\frac { \partial \mathbf { a } ^ { T } \mathbf { X } \mathbf { b } } { \partial \mathbf { X } } = \mathbf { a b } ^ { T }$
	\item $\frac { \partial \mathbf { a } ^ { T } \mathbf { X } ^ { T } \mathbf { b } } { \partial \mathbf { X } } = \mathbf { b a } ^ { T }$
	\item $\frac { \partial \mathbf { X } } { \partial X _ { i j } } = \mathbf { J } ^ { i j }$, $J^{ij}$ is the single entry matrix
	\item $\frac { \partial \mathbf { b } ^ { T } \mathbf { X } ^ { T } \mathbf { X } \mathbf { c } } { \partial \mathbf { X } } = \mathbf { X } \left( \mathbf { b } \mathbf { c } ^ { T } + \mathbf { c b } ^ { T } \right)$
	\item $\frac { \partial \mathbf { x } ^ { T } \mathbf { B } \mathbf { x } } { \partial \mathbf { x } } = \left( \mathbf { B } + \mathbf { B } ^ { T } \right) \mathbf { x }$
	\item $\frac { \partial } { \partial \mathbf { x } } ( \mathbf { x } - \mathbf { A } \mathbf { s } ) ^ { T } \mathbf { W } ( \mathbf { x } - \mathbf { A } \mathbf { s } ) = 2 \mathbf { W } ( \mathbf { x } - \mathbf { A } \mathbf { s } )$
	\item $\frac { \partial } { \partial \mathbf { X } } \| \mathbf { X } \| _ { \mathrm { F } } ^ { 2 } = \frac { \partial } { \partial \mathbf { X } } \operatorname { Tr } \left( \mathbf { X } \mathbf { X } ^ { H } \right) = 2 \mathbf { X }$
\end{itemize}

\subsection{Linear Algebra}

\begin{itemize}
    \item \textbf{positive definite} (pd) if $\*a^T \*V \*a > 0$
	\item $(\*x-\*b)^T (\*x-\*b) = \lVert \*x - \*b \lVert^2_2 $
	\item $\lVert \mathbf{X} \lVert_F =\lVert \mathbf{X}^T \lVert_F$
	\item $det X = x_{11}x_{22} - x_{12}x_{21}$, if X is $2 \times 2$
\end{itemize}

\subsection{Distributions}
Valid distribution $p(x) > 0 \text{, }\forall x$ and $\sum p(x) = 1$
Model is identifiable iff $\theta_1 = \theta_2 \rightarrow P_{\theta_1} = P_{\theta_2}$
\begin{itemize}
    \item \textbf{Gaussian} (Not convex): $ \N(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(- \frac{(x - \mu)^2}{2 \sigma^2})$
     $ \N(x | \mu, \Sigma^2) = \frac{\exp(-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu))}{\sqrt{ (2 \pi)^D det(\Sigma)}} $
    \item \textbf{Poisson}: $\text{P(k events in interval)} = e^{-\lambda} \frac{\lambda^k}{k!}$
    \item \textbf{Bernoulli}:  $p(y | \mu) = \mu^y (1- \mu)^{1 - y}$
\end{itemize}

\subsection{Convexity}

A function $f(x)$ is convex if
 
\begin{itemize}

    \item for any $\*x_1, \*x_2 \in \* X$ and $0 \leq \lambda \leq 1$, we have :
        \begin{myalign*}
            f(\lambda \*x_1 + (1 - \lambda) \*x_2) \leq \lambda f(\*x_1) + (1 - \lambda) f(\*x_2)
        \end{myalign*}
   \item it is a sum of convex functions
   \item composition of convex and linear functions
   \item $f(x) = g(h(x))$, g,h are convex, g increasing
   \item the Hessian $\*H$ is positive semi-definite
	
\end{itemize}

% ========================================================
% ========================================================
\section{Cost functions}

\textbf{Mean square error (MSE)}:
\begin{myalign*}
    MSE(\bm w ) = \frac{1}{N} \sum^N_{n = 1} (y_n - f(\* x_n ))^2
\end{myalign*}

\begin{itemize}
    \item MSE is \textbf{strictly convex} thus it has only one global minumum value.
    \item MSE is very prone to outliers.
\end{itemize}

\textbf{Mean Absolute Error (MAE)}:
\begin{myalign*}
    MAE = \frac{1}{N} \sum^N_{n = 1} |y_n - f(\* x_n)|
\end{myalign*}

\begin{itemize}
    \item MAE is more robust to outliers than MSE.
\end{itemize}

\textbf{Huber loss}
\begin{myalign*}
    Huber = 
    \left\{ 
        \begin{array}{c c}
            \frac{1}{2} z^2 &,|z| \leq \delta \\
            \delta |z| - \frac{1}{2} \delta^2 & ,|z| > \delta
        \end{array}
    \right.
\end{myalign*}
\begin{itemize}
\item Huber loss is convex, differentiable, and also robust to outliers but hard to set $\delta$.\\
\end{itemize}

\textbf{Tukey's bisquare loss}
\begin{myalign*}
    L(z) =
    \left\{ 
        \begin{array}{c c}
            z(\delta^2 - z^2)^2 &, |z| < \delta \\
            0 &, |z| \geq \delta
        \end{array}
    \right.
\end{myalign*}
Non-convex, non-diff., but robust to outliers.

\textbf{Hinge loss}: $[1 - y_n f(\* x_n)]_+ = \max(0, 1 - y_n f(\* x_n))$ \\
\textbf{Logistic loss}: $ \log(1 - \exp(y_n f(\* x_n)))$

% ========================================================
\section{Optimization}
\begin{itemize}
    \item Local minimum: $L(w^*) \leq L(w)~ \forall w: \Vert w-w^* \Vert < \epsilon$
    \item Global minimum: $L(w^*) \leq L(w)~ \forall w$
\end{itemize}

\subsection{Grid search}
\begin{itemize}
    \item Compute the cost over a grid of $V$ points. Exponential complexity $\mathcal{O}(|V|^D)$. Hard to find a good value range. No guarantee to converge.
\end{itemize}

% ========================================================

\subsection{GD - Gradient Descent (Batch)}
\begin{itemize}
	\item GD uses only first-order information
	\item Given cost function $\L(\*w)$ we want to find $\* w$
	\begin{myalign*}
	    \*w = \arg\min_{\*w} \L(\*w)
	\end{myalign*}
    \item Take steps in the opposite direction of gradient
   	\begin{myalign*}
	    \*w^{(t + 1)} \leftarrow \*w^{(t)} - \gamma \bm \nabla \L(\*w^{(t)})
   	\end{myalign*}

    \item With $\gamma$ too big, method might diverge. With $\gamma$ too small, convergence is slow.
    \item Very sensitive to ill-conditioning $\Rightarrow$ always normalize features $\Rightarrow$ allow different directions to converge at same speed.
\end{itemize}


% ========================================================

\subsection{SGD - Stochastic Gradient Descent}
SGD update rule (only n-th training example):
$$ \*w^{(t + 1)} \leftarrow \*w^{(t)} - \gamma \bm \nabla \L_n(\*w^{(t)})$$
\emph{Idea}: Cheap but unbiased estimate of grad. $$\E[\bm\nabla\L_n(\*w)] = \bm \nabla L(\*w)$$
\\
Robbins-Monroe condition:
\begin{itemize}
	\item $\gamma^{(t)}: \sum^{\infty}_{t=1} \gamma^{(t)} = \infty; \sum^{\infty}_{t=1} (\gamma^{(t)})^2 < \infty$
	\item e.g. $\gamma^{(t)} = 1 / (t+1)^r, r \in (0.5, 1)$
\end{itemize}

% ========================================================

\subsection{Mini-batch SGD}

Update direction ($B \subseteq [N]$):
$$\bm g^{(t)} := \frac{1}{|B|} \sum_{n\in B} \bm \nabla \L_n(\* w^{(t)}) $$ 
Update rule : $ \* w^{(t + 1)} \leftarrow \* w^{(t)} - \gamma \* g^{(t)}$ 

% ========================================================

\subsection{Gradients for MSE}
\begin{itemize}
    \item Define error $\*e := \* y - \*{X} \*w$
    \item and MSE as follows:
    \begin{myalign*}
        \L(\* w) = \frac{1}{2N} \sum^N_{n = 1} (\*y_n - \*{\tilde{x}}_n^T \* w)^2 = \frac{1}{2N} \*e^T \*e
    \end{myalign*}

    \item Optimality conditions:
    \begin{enumerate}
        \item \textit{necessary}: $\frac{d \L(\*w^*)}{d \*w} = - \frac{1}{N} \*{X}^T \*e = 0$
        \item \textit{sufficient}: Hessian matrix is positive definite: $\* H(\*w^*) = \frac{d^2 \L(\*w^*)}{d \*w d \*w^T} = \frac{1}{N} X^TX$
    \end{enumerate}
\end{itemize}



\subsection{Subgradients (Non-Smooth OPT)}

A vector $\* g\in \R^D$ s.t.
$$ \L(\* u) \geq \L(\* w) + \* g^T(\* u - \* w) \quad \forall \* u \in \R^D $$
is the subgradient to $\L$ at $\* w$.
If $\L$ is differentiable at $\* w$, we have $\* g = \bm \nabla \L(\* w)$
% ========================================================

\subsection{Constrained Optimization}
Find solution $\min{\L(\*w)}$ s.t. $\*w \in \mathcal{C}$

\begin{itemize}
    \item Add proj. onto $\mathcal{C}$ after each step:
    \begin{myalign*}
    	P_{\mathcal{C}}(\*w') = \arg\min \lvert \*v-\*w' \lvert \text{, } \*v \in\mathcal{C}
    \end{myalign*}
    \begin{myalign*}
       	\*w^{(t+1)} = P_{\mathcal{C}}[\*w^{(t)} - \gamma \nabla \L (\*w^{(t)})]
    \end{myalign*}
    \item Use penalty functions
    \begin{itemize}
    \item $ \min \L(\*w) + I_{\mathcal{C}}, I_{\mathcal{C}}=0\text{ if } \*w \in \mathcal{C} \text{, ow } +\infty$
    \item $\min \L(\*w) + \lambda \lvert \*A\*w - \*b \lvert$
    \end{itemize}
    \item Stopping criteria when $\L(\*w)$ close to 0
    
\end{itemize}

\subsection{Iteration complexities for MSE/MAE}
\begin{itemize}
    \item GD=$\mathcal{O}(ND)$
    \item MB-GD=$\mathcal{O}(BD)$
    \item SGD=$\mathcal{O}(D)$
\end{itemize}

% ========================================================

\section{Least Squares}
\begin{itemize}
   %\item In some cases, we can compute the minimum of the cost function analytically.

    \item Use the first optimality conditions:
    \begin{myalign*}
        \bm \nabla L(\* w^*)= 0 \Rightarrow \*X^T \*e = \*X^T (\*y - \*X \* w)=  0
    \end{myalign*}
    \item When $\*X^T\*X$ is invertible, we have the closed-form expression
    \begin{myalign*}
        \* w^* = (\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item thus we can predict values for a new $\* x_m$
    \begin{myalign*}
        \*y_m := \*{x^T_m} \* w^* = \*{x^T_m}(\*X^T\*X)^{-1} \*X^T \*y
    \end{myalign*}
    \item The \textbf{Gram matrix} $\*X^T\*X$ is pd and is also invertible iff $\*X$ has full column rank.
    
    \item \textit{Complexity}: $O(ND^2 + D^3) \equiv O(ND^2)$

    \item $\*X$ can be rank deficient when $D > N$ or when the columns $\*{\bar{x}}_d$ are nearly collinear. $\Rightarrow$  matrix is ill-conditioned.
    \item Can still solve using a linear system solver using normal equations:
    \begin{myalign*}
    	\*X^\top \*X\*w = \*X^\top \*y
    \end{myalign*}
\end{itemize}

% ========================================================

\section{Maximum Likelihood (MLE)} 
\begin{itemize}
    \item Let define the noise $\epsilon_n \sim \N(0, \sigma^2)$.
    \begin{myalign*}
        \rightarrow \*y_n = \*{x}_n^T \* w + \epsilon_n
    \end{myalign*}    
    \item Another way of expressing this:
    \begin{myalign*}
        p(\*y | \*{X, \* w}) &= \prod_{n = 1}^N p(\* y_n | \*{x}_n, \*w)\\
        &= \prod_{n = 1}^N \N(\*y_n | \*{x}_n^T \* w, \sigma^2)
    \end{myalign*}
    which defines the likelihood of observating $\* y$ given $\*X$ and $\bm w$
    \item Define cost with log-likelihood
    \begin{myalign*}
        \L_{MLE}(\bm w) &= \log p(\*y | \*X, \*w)\\
        &= - \frac{1}{2 \sigma^2} \sum^N_{n = 1} (\*y_n - \*{x}_n^T \*w)^2 + cnst
    \end{myalign*}
    \item Maximum likelihood estimator (MLE) gives another way to design cost functions
    \begin{myalign*}
        \argmin_{\* w} \L_{MSE}(\* w) = \argmax_{\*w} \L_{MLE}(\*w)
    \end{myalign*}
    \item MLE can also be interpreted as finding the model under which the observed data is most likely to have been generated from.
    \item $\*w_{\text{MLE}} \rightarrow \*w_{\text{true}}$ for large amount of data

\end{itemize}


% ========================================================

\section{Ridge Regression and LASSO}
\begin{itemize}
    \item Add \textbf{regularization term}
    \begin{myalign*}
        \min_{\bm w} \L(\* w) + \Omega(\*w) 
    \end{myalign*}    
    \item $L_2$-Reg. (Ridge): $\Omega(\*w) = \lambda \lVert \*w \lVert_2^2$
    \item $\rightarrow$ small values of $\*w_i$, not sparse
    \item $\rightarrow$ $\*w^{\star} = (\*X^T\*X + \lambda ' \*I)^{-1}\*X^T\*y$ with $\lambda ' = 2N\lambda$
    \item $\rightarrow$ $(\*X^T\*X + \lambda ' \*I)^{-1}$ exists (lifted eigenvalues)
    \item $L_1$-Reg. (Lasso): $\Omega(\*w) = \lambda \lVert \*w \lVert_1$
    \item $\rightarrow$ sparsity of weight vector
    \item $\rightarrow$ implicit model selection
    \item {\bf Maximum-a-posteriori (MAP)}
    \item (i) Posterior prob. $\propto$ Likelihood $\times$ Prior prob
    \begin{myalign*}
        p(\*y | \*X \*w) &= \prod^N \N(\*y_n | \*x_n^T \*w, \sigma_n^2) \\
        p(\*w) &= \N(\*w | 0, \sigma_0^2 \*I_D) \\
        \text{then} \rightarrow \*w^{\star} &= \argmax_{\*w} p(\*y | \*X \*w) \cdot p(\*w)
    \end{myalign*}   
    \begin{myalign*}
        \hspace{-.2cm} \*w^{\star} = \argmin_{\*w} \sum^N \frac{1}{2\sigma_n^2}(\*y_n - \*x^T \*w)^2 + \frac{1}{2\sigma_0^2} \lVert\*w \lVert^2
    \end{myalign*}   
\end{itemize}

% ========================================================

\section{Model Selection}
\begin{itemize}
    \item Generalisation error: $L_D(f)=\mathbb{E}[l(y,f(x))]$, but D normally unknown.
    \item Instead approximate by  $L_{S_{test}}(f_{train}) = \frac{1}{\vert S_{test} \vert} \sum_{S_{test}} l(y_n, f_{S_{train}}(x_n))$
    \item In expectation this equates the true error.
    \item Worst case, if comparing K models:
    \item $\mathbb{P}[\max_k\vert L_D(f_k) - L_{test}(f_k) \vert \geq \sqrt{\frac{(bia)^2\ln(2K/\delta)}{2\vert S_{test} \vert}}] \leq \delta$
    \item Error decreases as $\mathbb{O}(1/\sqrt{\vert S_{test} \vert})$
    \item Error only goes up by $\sqrt{\ln(K)}$ for testing K models.
    \item use cross-validation for an efficient, unbiased estimate of generalisation error and variance.
\end{itemize}

\section{Bias-Variance decomposition}
\begin{itemize}
	\item {\bf Simple} (e.g. large  $\lambda$) $\rightarrow$ large bias, but low variance
	\item {\bf Complex} (e.g. small  $\lambda$) $\rightarrow$ low bias, but large variance
	\item The expected squared loss between true model and learned model is a sum of three non-negative terms:

	 	$\mathbb{E}_S[(f(x) + \epsilon - f_{S}(x))^2] = Var[\epsilon]$ + bias + variance:
	\begin{itemize}
	 	\item \textbf{Bias} $= (f(x) - \mathbb{E}_{S'}[f_{S'}(x)])^2$: Difference between actual value and expected prediction.
	 	\item \textbf{Variance} $= \mathbb{E}_S[(\mathbb{E}_{S'}[f_{S'}(x)] - f_S(x)])^2]$: variance of predictions between training sets.
	\end{itemize}
	
	\item All terms are lower bounds for the error.
	\item Cannot do better than $Var[\epsilon]$.
	
\end{itemize}
% ========================================================

\section{Logistic Regression}

\begin{itemize}
	\item \textbf{Binary classifier}: use $y \in \{0, 1\}$.
	\item Can use least-squares to predict $\hat{y}_*$
	\begin{myalign*}
	    \hat{y} = 
	    \left\{
	    	\begin{array}{c c}		
	    		\*C_1 & \hat{y}_* < 0.5 \\
	    		\*C_2 & \hat{y}_* \geq 0.5 \\
	    	\end{array}		
	    \right.
	\end{myalign*}
	\item \textbf{Logistic function}
	\begin{myalign*}
	    \sigma(x) = \frac{\exp(x)}{1 + \exp(x)}
	\end{myalign*}
	\begin{myalign*}
	    p(\*y_n = 1| \*x_n) = \sigma( \*x^T \*w)\\
	    p(\*y_n = 0 | \*x_n) = 1 - \sigma( \*x^T \*w)
	\end{myalign*}
	\item The probabilistic model:
	\begin{myalign*}
	    p(\*y | \*X, \* w) = \prod_{n = 1}^N \sigma( \*x_n^T \* w)^{\*y_n}(1 - \sigma(\* x_n^T \* w))^{1 - \*y_n}
	\end{myalign*}
	\item The negative log-likelihood (w.r.t. MLE):
	\begin{myalign*}
	    \hspace{-1.3cm} \L(\* w) &= - \sum_{n = 1}^N \*y_n \ln \sigma(\*x_n^T \*w)  + (1-\*y_n)\ln(1- \sigma(\*x_n^T \*w))\\
	    &=  \sum_{n = 1}^N  \ln[1 + \exp(\*x_n^T \* w) ]-  \*y_n \*x_n^T\*w
	\end{myalign*}
	\item We can use the fact that
	\begin{myalign*}
	    \frac{d}{dz}\ln(1 + \exp(z)) = \sigma(z)
	\end{myalign*}
	\item Gradient of the log-likelihood %LEARN
	\begin{myalign*}
	    \*g = \bm \nabla \L (\* w) &= \sum_{n = 1}^N  \*x_n( \sigma(\* x_n^T \* w) - \*y_n) \\
	    &= \*X^T[\sigma(\*X \bm w) - \*y]
	\end{myalign*}
	\item The neg. log-likelihood $- \L_{mle}(\bm w)$ is convex
	\item \textbf{Hessian} of the neg. log-likelihood
	\begin{itemize}
		\item We know that
		\begin{myalign*}
		    \frac{d \sigma(t)}{dt} = \sigma(t)(1 - \sigma(t))
		\end{myalign*}
		\item Hessian is the derivative of the gradient
		\begin{myalign*}
		    \*H(\* w) &= \frac{d \*g(\* w)}{d \* w^T}  = \sum_{n = 1}^N \frac{d}{d \* w^T} \* x_n \sigma(\* x_n^T \* w) \\
		    &= \sum_{n = 1}^N \* x_n \* x_n^T \sigma(\* x_n^T \* w)(1 - \sigma(\* x_n^T \* w)) \\
		    &= \tilde{\*X}^T \*S \tilde{\*X}
		\end{myalign*}
		where $\*S$ is a $N \times N$ diagonal with
		\begin{myalign*}
		    S_{nn} = \sigma(\* x_n^T\* w)(1 - \sigma(\* x_n^T \* w))
		\end{myalign*}
		\item The neg. log-likelihood is not strictly convex. \textcolor{red}{????}
	\end{itemize}
	\item \textbf{Newton's Method} %LEARN
	\begin{itemize}
		\item Uses second-order information and takes steps in the direction that minimizes a quadratic approximation (Taylor)
		\begin{myalign*}
		    \L(\*w) = \L(\* w^{(k)}) + \*\nabla\L_k^T (\*w - \* w^{(k)})\\ + (\* w - \* w^{(k)})^T \*H_k(\* w - \* w^{(k)})
		\end{myalign*}
		and it's minimum is at
		\begin{myalign*}
		    \* w^{k + 1} =\* w^{(k)} - \gamma_k \*H_k^{-1}\*\nabla\L_k
		\end{myalign*}
		\item Complexity: $O((ND^2 + D^3)I)$
	\end{itemize}
	\item \textbf{Regularized Logistic Regression}
		\begin{itemize}
	    	\item If data is linearly separable, there is no best weight vector $\Rightarrow$ optimisation does not stop.
	    	\item $\rightarrow$ use penalty term.
	    \end{itemize}
	\begin{myalign*}
	    \argmin_{\*w} 
	    	- \sum_{n = 1}^N \ln p(\*y_n | \*x_n^T \* w) + \frac{\lambda}{2} \lVert \*w \lVert^2
	    \end{myalign*}
\end{itemize}

% ========================================================
\section{Exponential family distribution \& Generalized Linear Model}


\begin{itemize}
	  \item Exponential family distribution
		\begin{myalign*}
		    p(\*y | \bm \eta) = h(y) \exp(\bm \eta^T \bm \phi(\*y) - A(\bm \eta))
		\end{myalign*}
	  \item For proper normalisation ($\int p = 1$):
	  	\begin{myalign*}
		    A(\bm \eta) = \ln[\int_y h(y) \exp(\bm \eta^T \bm \phi(\*y))]
		\end{myalign*}
      \item {\bf Bernoulli} distribution example
      \begin{myalign*}
	  \rightarrow \exp( \log(\frac{\mu}{1 - \mu})y + \log(1 - \mu)))
      \end{myalign*}
      (i) \textbf{link function g} relates $\eta$ and $\mu$
      \begin{myalign*}
      \bm \eta = \*g(\bm \mu) \Leftrightarrow \bm \mu = \*g^{-1}(\bm \eta) \\
	  \eta = \log(\frac{\mu}{1 - \mu}) \leftrightarrow \mu = \frac{e^{\eta}}{1 + e^{\eta}}
      \end{myalign*}
      (ii) Note that $\mu$ is the mean parameter of $y$
      \item {\bf Gaussian} distribution example
      \begin{myalign*}
	  \exp( (\frac{\mu}{\sigma^2}, \frac{-1}{2\sigma^2})(y, y^2)^T - \frac{\mu^2}{2\sigma^2}-\frac{1}{2}\ln(2\pi \sigma^2))
      \end{myalign*}
      (i) link function
      \begin{myalign*}
	  \eta = (\eta_1 = \mu/\sigma^2, \eta_2 = -1/(2\sigma^2))^T
      \end{myalign*}
      \begin{myalign*}
      	  \mu = -\eta_1/(2\eta_2) \text{ ; } \sigma^2 = -1/(2\eta_2)
      \end{myalign*}
      
  \item First and second derivatives of $A(\eta)$ are related to the mean and the variance
  \begin{myalign*}
      \frac{d A(\eta)}{d \eta} = \E[\bm \phi(\eta)], \hspace{4pt} \frac{d^2 A(\eta)}{d \eta^2} = \V[\bm \phi(\eta)]
  \end{myalign*}
  \item $A(\eta)$ is convex
  \item The generalized maximum likelihood cost to minimize is
  \begin{myalign*}
      \min_{\* w} \L(\* w) = - \sum_{n = 1}^N \log(p(\*y_n | \*x^T_n \* w))
  \end{myalign*}
  where $p(\*y_n | \* x^T_n \* w)$ is an exponential family distribution
  \item We obtain the solution
  \begin{myalign*}
      \frac{d \L}{d \* w} = \*X^T[\*g^{-1}(\*X \*w ) - \bm \phi(\*y)]
  \end{myalign*}
\end{itemize}

% ========================================================

\section{k-Nearest Neighbor (k-NN)}
\begin{itemize}
	\item Performs best in low dimensions.
	\item Assumes close points have similar values 
	\item The k-NN regressor:
	\begin{myalign*}
	    f(\*x) = \frac{1}{k} \sum_{\*x_n \in nbh_k(\*x)} \*y_n
	\end{myalign*}
	\item The k-NN classifier:
	\begin{myalign*}
	    f(\*x) = modus\{x_n| \*x_n \in nbh_k(\*x)\}
	\end{myalign*}
	\item Large k $\rightarrow$ smoothing over large area
	\item Small k $\rightarrow$ averaging over small area
	\item \textbf{Curse of dimensionality}:
	\begin{itemize}
		\item[a)] Consider fixed fraction $\alpha$ of points and increase dimension $rightarrow$ need to explore almost whole range in each dimension.
		\item[b)] In high dimensions, points are far from each other $\Rightarrow$ choice of NN becomes essentially random. \\
		          Need radius \\
		          $r = \sqrt[\leftroot{-2}\uproot{2}d]{(1-\frac{1}{\sqrt[\leftroot{-2}\uproot{2}N]{2}})}$ \\
		          to have at least one data point in $r^d$ rectangle with $p \geq \frac{1}{2}$.
	\end{itemize}
	\item \textbf{NN performance}:
	\begin{itemize}
		\item Bayes classifier:$ f_*(x) = \mathbbm{1}\{\mathbbm{P}[y=1|x] > \frac{1}{2}\}$
		\item $\mathbb{E}_S[L(f_S)] \leq 2 L(f_*) + 4c\sqrt{d}N^{\frac{-1}{1+d}}$
	\end{itemize}
\end{itemize}

% ========================================================

\section{Support Vector Machine} 
\begin{itemize}
	\item Assume $y_n \in \{-1, 1\}$ and optimise
	\begin{myalign*}
	    \L(\* w) = \min_{\* w} \sum_{n = 1}^N [1 - \*y_n x_n^T \* w]_+ + \frac{\lambda}{2} \lVert \*w \lVert^2
	\end{myalign*}
	\item Can be optimised using subgradient descent.
	\item \textbf{Case: Linear separability:} We get a seperating hyperplane, no point in the margin and w, s.t margin is maximised ($2/\Vert w \Vert$).
	\item This is called hard-margin compared to soft-margin formulation.
	\item \textbf{Duality}:
	\begin{itemize}
		\item Hard to minimize $g(\* w)$ so we define
		\begin{myalign*}
		    \L(\* w) = \max_{\bm \alpha} G(\* w, \bm \alpha)
		\end{myalign*}
		\item we use the property that
		\begin{myalign*}
		    [\*v_n]_+ = \max(0, \*v_n) = \max_{\alpha_n \in [0, 1]} \alpha_n \*v_n
		\end{myalign*}
		\item We can rewrite the problem as
		\begin{myalign*}
		    \min_{\* w} \max_{\alpha} \sum_{n = 1}^N \alpha_n (1 - \*y_n \bm \phi_n^T \* w) + \frac{\lambda}{2} \lVert \*w \lVert_2^2 
		\end{myalign*}
		\item This is differentiable, convex in $\bm w$ and concave in $\bm \alpha$
		\item \textbf{Minimax theorem}: 
		\begin{myalign*}
		    \min_{\* w} \max_{\bm \alpha} G(\* w, \bm \alpha) = \max_{\bm \alpha} \min_{\* w} G(\* w, \bm \alpha)
		\end{myalign*}
		because $G$ is convex in $\* w$ and concave in $\bm \alpha$.
		\item Derivative w.r.t. $\* w$:
		\begin{myalign*}
		    \bm \nabla_{\* w} G(\* w, \bm \alpha) = - \sum_{n=1}^N\alpha_n \*y_n \* x_n + \lambda \* w
		\end{myalign*}
		\item Equating this to 0, we get:
			\begin{myalign*}
			  \* w(\bm \alpha) = \frac{1}{\lambda} \sum_{n=1}^N \alpha_n \*y_n \*x_n = \frac{1}{\lambda} \*{X}^T\*{Y}\bm \alpha \\
			  \*{Y} := \text{diag}(\bm y)
			\end{myalign*}
		\item Plugging $\* w^*$ back in the dual problem
		\begin{myalign*}
		    \max_{\bm \alpha \in [0, 1]^N} \bm \alpha^T \*1 - \frac{1}{2\lambda} \bm \alpha^T \*Y \* X  \*X^T \* Y \bm \alpha
		\end{myalign*}
		\item Data only enters as $\*K = \bm X^T \bm X$.
	\end{itemize}
\end{itemize}

\begin{itemize}
	\item {\bf Non support vector}: Example that lies on the correct side, outside margin $\bm \alpha_n = 0$
	\item {\bf Essen. support vector}: Example that lies on the margin $\bm \alpha_n \in (0,1)$
	\item {\bf Bound support vector}: Example that lies strictly inside the margin or wrong side $\bm \alpha_n = 1$
	\item Use Coordinates ascent to find $\bm \alpha$. Update one coordinate ($\argmin$) at the time and others constant.
\end{itemize}

% ========================================================

\section{Kernel Ridge Regression}
\begin{itemize}
	\item The following is true for ridge regression
	\begin{myalign*}
	    \* w^* &= (\*X^T \*X + \lambda \*I_D)^{-1} \*X^T \*y \text{ , (1)}\\
	    &= \*X^T(\*X\*X^T + \lambda \*I_N)^{-1} \*y = \*X^T \bm \alpha^* \text{ , (2)}
	\end{myalign*}
	\item Complexity of computing $\* w$: (1) $O(D^2 N + D^3)$, (2) $O(D N^2 + N^3)$
	\item Thus we have
	\begin{myalign*}
	     \*w^* = \*X^T\bm \alpha^*, \quad \text{with } \* w^* \in \R^D \text{ and } \bm \alpha^* \in \R^N
	\end{myalign*}
	\item Following representer theorem write:
	\begin{myalign*}
	    \bm \alpha = \argmax_{\bm \alpha} 
	    \left(
	    - \frac{1}{2}\bm \alpha^T (\*X^T \*X + \lambda \*I_N) \bm \alpha + \bm \alpha^T \*y 
	    \right)
	\end{myalign*}
	\item $\*K = \*X \*X^T$ is called the \textbf{kernel matrix} or \textbf{Gram matrix}.
	\item If $\*K$ is positive definite and symmetric, then it's called a \textbf{Mercer Kernel}.
	\item $\*K_{i,j} = k(\*x_i, \*x_j)$
	\item If the kernel is Mercer, then there exists a function $\bm \phi(\*x)$ s.t.
	\begin{myalign*}
	    k(\*x, \*x') = \bm \phi(\*x)^T \bm \phi(\*x')
	\end{myalign*}
	\item \textbf{Kernel trick}: 
	\begin{itemize}
		\item compute dot-product in $\mathbb{R}^m$ while remaining in $\mathbb{R}^n$
		\item Replace $\langle \*x, \*x' \rangle$ with $k(\*x, \*x')$.
	\end{itemize}
	
	\item \textbf{Common Kernel}
	\begin{itemize}
		\item $x\in\mathbb{R}, k(\*x, \*x') = (xx')^2 \Rightarrow \phi(x)=x^2$
		\item Radial Basis function kernel (RBF)
			\begin{myalign*}
	    			 k(\*x, \*x') = \exp(- \frac{1}{2}(\*x - \*x')^T (\*x - \*x'))
			 \end{myalign*}  
	\end{itemize}
	 
	\item Thus we get
	\begin{myalign*}
	    \*y = \*w^T \*x = \sum_{i = 1}^K \alpha_i \*x_i^T \*x = \sum_{i = 1}^K \alpha_i k(\*x, \*x_i) 
	\end{myalign*}
	\item \textbf{Creating new kernels:}
	\begin{itemize}
		\item $\kappa(x,x') = a\kappa_1(x, x') + b\kappa_2(x, x')$
		\item $\kappa(x,x') = \kappa_1(x, x') \kappa_2(x, x')$
		\item $\kappa(x,x') = \kappa_1(f(x), f(x'))$
	\end{itemize}
\end{itemize}

\section{K-means}
\begin{myalign*}
	    \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = \sum_{k = 1}^K \sum_{n = 1}^N z_{nk} ||\*x_n - \bm \mu_k||^2_2
	\end{myalign*}
	such that $z_{nk} \in \{0, 1\}$ and $\sum_{k = 1}^K z_{nk} = 1$ 
\begin{itemize}
	\item K-means algorithm (Coordinate Descent): \\
	Initialize $\bm \mu_k$, then iterate
	\begin{enumerate}
		\item For all n, compute $\*z_n$ given $\bm \mu$
		\begin{myalign*}
		    z_{nk} = 
		    \left\{
		    	\begin{array}{c c}
		    		1 & \text{ if } k = \argmin_j || \*x_n - \bm \mu ||^2_2\\
		    		0 & \text{otherwise}
		    	\end{array}
		    \right.
		\end{myalign*}
		\item For all $k$, compute $\mu_k$ given $\*z$
		\begin{myalign*}
		    \bm \mu_k = \frac{\sum_{n = 1}^N z_{nk} \*x_n}{\sum_{n = 1}^N z_{nk}}
		\end{myalign*}
	\end{enumerate}
	\item A good initialization procedure is to choose the prototypes to be equal to a random subset of $K$ data points.
	\item Probabilistic model
	\begin{myalign*}
	    & p(\*z, \bm \mu) = \prod_{n = 1}^N \prod_{k = 1}^K 
	    \left[
	    	\N(\*x_n | \bm \mu_k, \*I)
	    \right]^{z_{nk}} \\ 
	     & \hspace{-.5cm} -\log{p(\* x_n | \mu, z)}= \sum^N \sum^K \frac{1}{2} \lVert \*x_n - \mu_k \lVert^2 \*z_{nk} + c'
	\end{myalign*}
	\item K-means as a Matrix Factorization
	\begin{myalign*}
	 \min_{\*z, \bm \mu} \L(\*z, \bm \mu) = ||\*X - \*M\*Z^T||_{\text{Frob}}^2
	\end{myalign*}
	\item Computation can be heavy, each example can belong to only on cluster and clusters have to be spherical.
\end{itemize}

% ========================================================


\section{Gaussian Mixture Models}
\begin{itemize}
	\item Clusters can be elliptical using a full instead of isotropic covariance matrix.
	\begin{myalign*}
	    p(\*X | \bm \mu, \bm \Sigma, \*z) = \prod_{n = 1}^N \prod_{k = 1}^K 
		    \left[
		    	\N(\*x_n | \bm \mu_k, \* \bm \Sigma_k)
		    \right]^{z_{nk}}
	\end{myalign*}

	\item \textbf{Soft-clustering}: Points can belong to several cluster by defining $z_n$ to be a random variable.
	\begin{myalign*}
		p(z_{n} = k) &= \pi_k \text{ where } \pi_k > 0, \forall k, %LEARN
		\sum_{k = 1}^K \pi_k = 1
	\end{myalign*}
	\item Joint distribution of Gaussian mixture model
	\begin{myalign*}
	    p(\*X, \*z | \bm \mu, \bm \Sigma, \bm \pi)
	    = \prod_{n = 1}^N
	    	p(\*x_n | \*r_n, \bm \mu, \bm \Sigma) p(\*z_n | \bm \pi) \\
	    = \prod_{n=1}^N \prod_{k = 1}^K [(\N(\*x_n |\bm \mu_k, \bm \Sigma_k))^{z_{nk}}] \prod_{k = 1}^K [\pi_k]^{z_{nk}}
	\end{myalign*}
	\item $z_n$ are called \textit{latent} unobserved variables
	\item Unknown parameters are $\bm \theta = \{\bm \mu, \bm \Sigma, \bm \pi\}$
	\item We get the \textbf{marginal likelihood} by marginalizing $z_n$ out from the likelihood
	\begin{myalign*}
	    p(\*x_n | \bm \theta) &= \sum_{k = 1}^K p(\*x_n, z_{n} = k | \bm \theta)\\
	    &= \sum_{k = 1}^K p(z_{n} = k | \bm \theta) p(\*x_n | z_{n} = k, \bm \theta)\\
	    &= \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
	\item Without a latent variable model, number of parameters grow at rate $O(N)$
	\item After marginalization, the growth is reduced to $O(D^2 K)$

	\item To get maximum likelihood estimate of $\bm \theta$, we maximize
	\begin{myalign*}
	    \max_{\bm \theta} \sum_{n = 1}^N \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)
	\end{myalign*}
\end{itemize}

% ========================================================

\section{Expectation Maximization Algorithm} %LEARN

\begin{itemize}
	\item \textit{[ALGORITHM]} Start with $\bm \theta^{(1)}$ and iterate
	\begin{enumerate}
		\item \textit{Expectation step}: Compute a lower bound to the cost such that it is tight at the previous $\bm \theta^{(t)}$
		\begin{comment}
		\begin{myalign*}
		    \log \sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k) \geq \sum_{k = 1}^K \gamma(r_{nk}) \log \frac{\pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}{\gamma(r_{nk})}
		\end{myalign*}
		\end{comment}
		with equality when,
		\begin{myalign*}
		    q_{kn} = \frac{\pi_k \N(\*x_n| \bm \mu_k, \bm \Sigma_k)}{\sum_{k = 1}^K \pi_k \N(\*x_n | \bm \mu_k, \bm \Sigma_k)}
		\end{myalign*}
		\item \textit{Maximization step}: Update $\bm \theta$
		\begin{myalign*}
		    \bm \theta^{(t + 1)} = \argmax_{\bm \theta} \L(\bm \theta, \bm \theta^{(t)})
		\end{myalign*}

		\begin{myalign*}
		    \bm \mu_k^{(t + 1)} = \frac{\sum_{n = 1}^N \gamma^{(i)}(r_{nk}) \*x_n}{\sum_{n = 1}^N q_{kn}^{(t)}}
		\end{myalign*}

		\begin{myalign*}
			\hspace{-.88cm}
		    \bm\Sigma_k^{(t + 1)} = \frac{\sum_{n = 1}^N q_{kn}^{(t)} (\*x_n - \bm \mu_k^{(t + 1)})(\*x_n - \bm \mu_k^{(t + 1)})^T}{\sum_{n = 1}^N q_{kn}^{(t)})}
		\end{myalign*}

		\begin{myalign*}
		    \pi_k^{(t + 1)} = \frac{1}{N} \sum_{n = 1}^N q_{kn}^{(t)}
		\end{myalign*}
	\end{enumerate}
	\item If covariance is diagonal $\rightarrow$ K-means.
	\item $q_{nk}^{(t)} = p(z_n = k | x_n, \theta^{(t)})$ posterior of $z_n$
\end{itemize}

% ========================================================

\section{Matrix factorization}
\begin{itemize}
	\item Find $\*X \approx \*W \*Z^\top$
	 \begin{itemize}
		\item $\*X$ is $D \times N$ (e.g movies $\times$ user)
		\item $\*Z$ is $N \times K$, $\*W$ is $D \times K$ matrix
	 \end{itemize} 
	 \begin{myalign*}
	     \L(\*W, \*Z) = \frac{1}{2}\sum_{(d,n)\in\Omega} [x_{dn} - (\*W\*Z^T)_{dn}]^2 \\ + \frac{\lambda_w}{2} \lVert \*W \lVert_{\text{Frob}}^2 + \frac{\lambda_z}{2} \lVert \*Z \lVert_{\text{Frob}}^2 
	 \end{myalign*}
	 \item {\bf SGD}: For one fixed element $(d,n)$ we derive entry $(d',k)$ of $\*W$ (if $d=d'$ oth. 0):
	 \begin{myalign*}
		\frac{\partial}{\partial w_{d',k}} f_{d,n}(\*W, \*Z)  = -[x_{dn}-(\*W\*Z^T)_{dn}]z_{nk}
	 \end{myalign*}
	 And of $\*Z$ (if $n=n'$ oth. 0):
	 \begin{myalign*}
	 	\frac{\partial}{\partial z_{n',k}} f_{d,n}(\*W, \*Z)  = -[x_{dn}-(\*W\*Z^T)_{dn}]w_{nk}
	 \end{myalign*}
	 updates:
	 \begin{myalign*}
  		\*W^{t+1} &= \*W^t - \gamma \nabla_w f_{d,n}(\*W^t, \*Z^t) \\
 	 	\*Z^{t+1} &= \*W^t - \gamma \nabla_z f_{d,n}(\*W^t, \*Z^t)
	 \end{myalign*}
	 \item We can use coordinate descent algorithm, by first minimizing w.r.t. $\*Z$ given $\*W$ and then minimizing $\*W$ given $\*Z$. This is called \textbf{Alternating least-squares (ALS)}:
	 \begin{myalign*}
	     \*Z^T &\leftarrow (\*W^T \*W + \lambda_z \*I_K)^{-1} \*W^T \*X \\
	     \*W^T &\leftarrow (\*Z^T \*Z + \lambda_w \*I_K)^{-1} \*Z^T\*X^T
	 \end{myalign*}
	 \item $\mathbb{O}(D N K + D K^2)$ and $\mathbb{O}(D N K + N K^2)$

\end{itemize}
% ========================================================

\section{Text Representation}
\begin{itemize}
	\item \textbf{word2vec}: map every word to a vector $w_i \in \mathbb{R}^K$, K large, that captures its semantics.
	\item \textbf{Topic model}: Documents consist of collections of topics
		\begin{itemize}
			\item topic = probability distribution over words
			\item use clustering to pick out respresentative topics
		\end{itemize}
	\item \textbf{Word representations by matrix factorisation}
	\item typically use log counts from co-occurance matrix
	\item $\min_{w,z} L(w,z) = \frac{1}{2} \sum_{(d,n)\in\Omega} f_{dn} [x_{dn} - (WZ^\top)_{dn}]^2$
	\item $f_{dn}$: importance of entry
	\item $f_{dn} = 1$ is okay, but better $f_{dn} = \min[1, (n_{dn}/N_{max}^\alpha], \alpha \in [0,1], n_{dn}$ are counts.
	\item this weighting is called GloVe (word2vec variant) and creates spatial analogies
	\item training with SGD or ALS
	\item Skip-Gram (original word2vec) uses binary classification to distinguish real from fake word pairs. Implicitly based on matrix factorisation.
	\item FastText: supervised sentence classification.
	\begin{itemize}
		\item Sentence as $x_n$ bag-of-words representation, f is a linear classifier loss, $y_n \in \{0,1\}$
		\item $\min_{W,Z} L(W,Z) = \sum_{x_n} f(y_n, WZ^\top x_n), W is 1 \times K, Z \vert V \vert \times K$ 
	\end{itemize}
\end{itemize}

\section{Singular Value Decomposition}
\begin{itemize}
	\item Matrix factorization method $\*X = \*U \*S \*V^T$
	\begin{itemize}
		\item $\*U$ orthonormal $D \times D$, $\*V$ orthonormal $N \times N$
		\item $\*S$ contains (non-negative) singular values in diagonal in descending order: $D \times N$
		\item Columns of $\*U$ and $\*V$ are the left and right \textbf{singular vectors} (eigenvectors of $\*X\*X^\top$ and $\*X^\top\*X$).
	\end{itemize}
	\item {\bf Truncated SVD}: \\
	Take the matrix $\*S^{(K)}$ with the $K$ first diagonal elements non zero.	
	\begin{myalign*}
		\*X \approx \*X_K = \*U\*S^{(K)}\*V^T
	\end{myalign*}
	
\end{itemize}
% ========================================================

\section{Principal Component Analysis}
\begin{itemize}
	\item dimensionality reduction and decorrelation
$\Vert\*X - \hat{\*X}\Vert_F^2 \geq \Vert \*X - \*U_k\*U_k^\top\*X \Vert_F^2 = \sum_{i>K}s_i^2$
\item If the data has zero mean
\begin{myalign*}
    \*\Sigma = \frac{1}{N} \*X \*X^T &\Rightarrow \*X \*X^T = \*U \*S^2 \*U^T \\
    \Rightarrow \*U^T \*X \*X^T \*U &= \*U^T \*U \*S^2 \*U^T \*U = \*S^2\\
\end{myalign*}
\item Columns of $\*U$ are called \textbf{principal components} and decorrelate $\*X$'s columns.
\item Not invariant under scalings $\rightarrow$ normalize $\*X$
\item Can compute U and S efficiently via $EVD(\*X\*X^\top)$ or $EVD(\*X^\top\*X)$

\end{itemize}



\section{Neural Net}
\begin{itemize}
	\item $x_j^{(l)} = \phi\left(\sum_i w_{i,j}^{(l)}x_i^{(l-1)} + b_j^{(l)} \right)$.
	\item NN with one hidden layer and sigmoid-like activation function can approximate any sufficiently smooth function on a bounded domain in average ($\leq\frac{(2Cr)^2}{n}$) and point-wise
	\item Cost function: \\$\L = \frac{1}{N} \sum_{n=1}^N\left( y_n - f^{(L+1)} \circ ... \circ f^{(1)}(\bm x_n^{(0)})\right)^2$ \\
	We can use SGD to minimize the cost.
\end{itemize}

% ========================================================

\subsection{Backpropagation Algorithm}
\begin{itemize}
 \item \emph{Forward pass}: Compute $\bm z^{(l)} = \left(\bm W^{(l)}\right)^T\bm x^{(l-1)} + \bm b^{(l)}$ with $\bm x^{(0)} = \bm x_n$ and $\bm x^{(l)} = \phi(\bm z^{(l)})$.
 \item \emph{Backward pass}: Set $\delta^{(L+1)} = -2(y_n - \bm x^{(L+1)})\phi'(z^{(L+1)})$ (if squared loss). Then compute
 
\end{itemize}
 \begin{myalign*}
  \delta_j^{(l)} = \frac{\partial \L_n}{\partial z_j^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l+1)}}\frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} \\= \sum_k \delta_k^{(l+1)}\bm W_{j,k}^{(l+1)}\phi'(z_j^{(l)}) 
 \end{myalign*}
\begin{itemize}
 \item \emph{Final Computation}:
\end{itemize}
\begin{myalign*}
\frac{\partial\L_n}{\partial w_{i,j}^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l)}}\frac{\partial z_k^{(l)}}{\partial w_{i,j}^{(l)}} = \frac{\partial \L_n}{\partial z_j^{(l)}}\frac{\partial z_j^{(l)}}{\partial w_{i,j}^{(l)}} \\= \delta_j^{(l)}\bm x_i^{(l-1)}
\end{myalign*}
\begin{myalign*}
\frac{\partial \L_n}{\partial b_j^{(l)}} = \sum_k \frac{\partial \L_n}{\partial z_k^{(l)}}\frac{\partial z_k^{(l)}}{\partial b_j^{(l)}} = \frac{\partial \L_n}{\partial z_j^{(l)}}\frac{\partial z_j^{(l)}}{\partial b_j^{(l)}} \\= \delta_j^{(l)}\cdot 1 = \delta_j^{(l)} 
\end{myalign*}
 
% ======================================================== 
 
\subsection{Activation Functions}
\begin{description}
 \item[Sigmoid] $\phi(x) = \frac{1}{1+e^{-x}}$ Positive, bounded. $\phi'(x) \simeq 0$ for large $|x|$ $\Rightarrow$ Learning slow.
 \item[Tanh] $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \phi(2x) - 1/2$. Balanced, bounded. Learning slow too.
 \item[ReLU] $(x)_{+} = \max{0,x}$ Positive, unbounded. Derivate = 1 if $x>0$, 0 if $x<0$
 \item[Leaky ReLU] $f(x) = \max{\alpha x, x}$ Remove 0 derivative.
 \item[Maxout] $f(x) = max{\bm x^T\bm w_1 + b_1, ..., \bm x^T\bm w_k + b_k}$ (Generalization of ReLU)
\end{description}

% ======================================================== 
 
\subsection{Convolutional NN} 

Sparse connections and \textit{weights sharing}: reduce parameters.

% ======================================================== 
 
\subsection{Reg, Data Augmentation and Dropout}

\begin{itemize}
	\item Regularization term: $\frac{1}{2} \sum_{l=1}^{L+1} \mu^{(l)} || W ^{(l)} ||  ^{2} _{F}$
	\item Weight decay is $\Theta[t](1-\eta \mu)$ in:
	\begin{myalign*}
	\Theta[t+1] = \Theta[t] + \eta (\nabla \L + \mu \Theta[t])
	\end{myalign*}
	\item Data Augm.: e.g. shift or rotation of pics
	\item Dropout: avoid overfit. Drop nodes randomly. (Then average multiple drop-NN or divide by dropout rate.)
\end{itemize}

% ========================================================

\section{Bayes Net}
\begin{itemize}
	\item Graph example: $p(x, y, z) =  p(x) p(y | x) p(z | x)$ : $(y \leftarrow x \rightarrow z)$ 
	\item \textbf{D-Separation} X and Y are D-separated by Z if every path from $ x \in X$ to $y \in Y$ is blocked by Z. ($\rightarrow$ independent)
	\item \textbf{Blocked Path} contains a variable that
	\begin{itemize}
		\item is in Z and is \textbf{head-to-tail} or \textbf{tail-to-tail}
		\item the node is \textbf{head-to-head} and neither the node nor any of its descendants are in Z.
	\end{itemize}	
	\item \textbf{Markov Blanket} (which blocks node A from the rest of the net) contains:
	\begin{itemize}
		\item parents of A
		\item children of A
		\item parents of children of A
	\end{itemize}
	
\end{itemize}






